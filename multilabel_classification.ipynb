{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 20), (10000, 5))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n_labels - average number of labels per instance\n",
    "\n",
    "n_samples = 10000\n",
    "n_features = 20\n",
    "n_classes = 5\n",
    "n_labels = 3\n",
    "\n",
    "X, y = make_multilabel_classification(n_samples = n_samples, n_features = n_features, \n",
    "                                      n_classes = n_classes, n_labels = n_labels, random_state = 0)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 20), (2000, 20), (8000, 5), (2000, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of exactly correct predictions: 28.95 %\n"
     ]
    }
   ],
   "source": [
    "def get_exact_match(y_test, y_pred):\n",
    "    return np.sum(np.sum(y_pred==y_test, axis=1)==5) / len(y_test)\n",
    "\n",
    "exact_match = get_exact_match(y_test, y_pred)\n",
    "\n",
    "print('Percentage of exactly correct predictions:', exact_match * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is difficult to get an exactly correct prediction i.e. all predicted labels for an example match with ground-truth labels (measured through exact-ratio), we look for metrics which award the model for partially correct results.\n",
    "\n",
    "1. **Macro and micro measures of Precision and Recall**\n",
    "2. **Hamming Loss** = 1 - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2457, 0.7543)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_hamming_loss(y_test, y_pred):\n",
    "    return np.sum(y_test!=y_pred) / (len(y_test) * n_classes)\n",
    "\n",
    "hamming_loss = get_hamming_loss(y_test, y_pred)\n",
    "accuracy = 1 - hamming_loss\n",
    "\n",
    "hamming_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that the exact match is roughly **29%** whereas accuracy of identifying labels correctly is **75%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(y_true, y_pred):\n",
    "    true_pos = np.sum((y_true==y_pred) & (y_pred==1))\n",
    "    pred_pos = np.sum(y_pred==1)\n",
    "    \n",
    "    if pred_pos == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_pos / pred_pos \n",
    "    \n",
    "def calculate_recall(y_true, y_pred):\n",
    "    true_pos = np.sum((y_true==y_pred) & (y_pred==1))\n",
    "    actual_pos = np.sum(y_true==1)\n",
    "    \n",
    "    if actual_pos == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_pos / actual_pos  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_metrics(y_true, y_pred, n_classes):\n",
    "    \n",
    "    precs=[]\n",
    "    recs=[]\n",
    "    \n",
    "    for c in range(n_classes):    \n",
    "        precs.append(calculate_precision(y_true[:,c], y_pred[:,c]))\n",
    "        recs.append(calculate_recall(y_true[:,c], y_pred[:,c]))\n",
    "\n",
    "    return np.mean(precs), np.mean(recs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7448646326986749, 0.8154168705329793, 0.7785456489843088)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_prec, macro_rec = get_macro_metrics(y_test, y_pred, n_classes)\n",
    "\n",
    "macro_f1 = 2 * macro_prec * macro_rec / (macro_prec + macro_rec)\n",
    "\n",
    "macro_prec, macro_rec, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(y_true, y_pred):\n",
    "    \n",
    "    tp = np.sum((y_true==y_pred) & (y_pred==1))\n",
    "    \n",
    "    tn = np.sum((y_true==y_pred) & (y_pred!=1))\n",
    "    \n",
    "    fp = np.sum((y_true!=1) & (y_pred==1))\n",
    "    \n",
    "    fn = np.sum((y_true==1) & (y_pred!=1))\n",
    "    \n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in case of multiple classes, there aren't positive or negative classes. And so, to calculate micro-measures, we need to look for correct and incorrect predictions only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_incorrect_predictions(y_true, y_pred):\n",
    "    return np.sum(y_true==y_pred), np.sum(y_true!=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7543, 2457)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes=[]\n",
    "\n",
    "for c in range(n_classes):  \n",
    "    c, inc = correct_incorrect_predictions(y_test[:,c], y_pred[:,c])\n",
    "    outcomes.append([c, inc])\n",
    "\n",
    "outcomes=np.array(outcomes)\n",
    "\n",
    "C, INC = np.sum(outcomes, axis=0)\n",
    "C, INC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_micro_metrics(y_true, y_pred, n_classes):\n",
    "    \n",
    "    outcomes=[]\n",
    "\n",
    "    for c in range(n_classes):  \n",
    "        c, inc = correct_incorrect_predictions(y_test[:,c], y_pred[:,c])\n",
    "        outcomes.append([c, inc])\n",
    "\n",
    "    outcomes=np.array(outcomes)\n",
    "\n",
    "    C, INC = np.sum(outcomes, axis=0)\n",
    "    \n",
    "    actuals = len(y_true) * n_classes\n",
    "    predictions = len(y_pred) * n_classes\n",
    "    \n",
    "    if (predictions==0):\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = C / predictions\n",
    "        \n",
    "    if (actuals==0):\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = C / (actuals)\n",
    "        \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7543, 0.7543, 0.7543, 0.7543)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_prec, micro_rec = get_micro_metrics(y_test, y_pred, n_classes)\n",
    "\n",
    "micro_f1 = 2 * micro_prec * micro_rec / (micro_prec + micro_rec)\n",
    "\n",
    "micro_prec, micro_rec, micro_f1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We notice that micro measures (precision, recall and F1) are all equal to each other and to accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
